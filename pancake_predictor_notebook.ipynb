{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PANCAKE PREDICTOR: BNB 5-MINUTE MODEL\n",
    "\n",
    "**A Comprehensive Notebook \u2014 Two Models, Comparison, Ensemble & Knowledge Distillation**\n",
    "\n",
    "This notebook is **fully self-contained**: it installs all dependencies and runs on its own.\n",
    "\n",
    "---\n",
    "\n",
    "## CRISP-DM Workflow\n",
    "\n",
    "### 1. Business Understanding\n",
    "- **The Game**: Bet Bull (Up) or Bear (Down) for a 5-minute timeframe on PancakeSwap.\n",
    "- **The Constraint**: Must place the bet before the round locks.\n",
    "- **The Edge**: BiLSTMs detect momentum shifts invisible to the naked eye.\n",
    "- **The House Edge**: PancakeSwap takes ~3%. Model needs >53% accuracy to break even, or play Positive Expected Value.\n",
    "\n",
    "### 2. Data Understanding & Preparation\n",
    "- **Market Data**: 1-minute OHLCV candles for BNB/USDT (simulated).\n",
    "- **Contract Data**: Crowd sentiment (Bull Payout vs. Bear Payout).\n",
    "- **Feature Engineering**: RSI, volume spikes, sentiment ratios.\n",
    "\n",
    "### 3. The Architectures (DLA Based)\n",
    "\n",
    "| Feature | Base Model | Robust Model |\n",
    "|---|---|---|\n",
    "| Noise Injection | \u2717 | \u2713 GaussianNoise(0.05) |\n",
    "| Feature Extraction | \u2014 | Conv1D(32, kernel=3) |\n",
    "| BiLSTM Layers | 1 | 2 (Stacked) |\n",
    "| Attention | MultiHeadAttention | MultiHeadAttention + Residual |\n",
    "| Dropout | 0.2 | 0.3 |\n",
    "| Learning Rate | Adam default | Adam 0.0005 |\n",
    "\n",
    "### 4. Evaluation\n",
    "- Side-by-side training comparison\n",
    "- Ensemble prediction (weighted average)\n",
    "- Knowledge Distillation (teacher \u2192 student)\n",
    "- Expected Value (EV) based trading logic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Install Dependencies\n",
    "\n",
    "Run this cell to install all required packages. This ensures the notebook runs on its own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all required dependencies\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install(package):\n",
    "    subprocess.check_call([sys.executable, '-m', 'pip', 'install', '-q', package])\n",
    "\n",
    "install('tensorflow>=2.10.0')\n",
    "install('numpy>=1.21.0')\n",
    "install('pandas>=1.3.0')\n",
    "install('scikit-learn>=1.0.1')\n",
    "install('matplotlib>=3.4.0')\n",
    "\n",
    "print('All dependencies installed successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# PANCAKE PREDICTOR: BNB 5-MINUTE MODEL\n",
    "# ==========================================\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras.layers import Conv1D, GaussianNoise, Add, LayerNormalization, GlobalAveragePooling1D\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "SEQ_LENGTH = 30       # Look back at last 30 minutes\n",
    "PREDICT_AHEAD = 5     # Predict 5 minutes into the future\n",
    "FEATURES = 5          # [Close, Volume, RSI, Bull_Ratio, Bear_Ratio]\n",
    "\n",
    "print(f'TensorFlow version: {tf.__version__}')\n",
    "print(f'NumPy version: {np.__version__}')\n",
    "print(f'Pandas version: {pd.__version__}')\n",
    "print(f'Configuration: SEQ_LENGTH={SEQ_LENGTH}, PREDICT_AHEAD={PREDICT_AHEAD}, FEATURES={FEATURES}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Generator (Simulating BNB Price & Contract Data)",
    "",
    "We simulate 1-minute OHLCV candles with PancakeSwap pool sentiment.",
    "In production, uses real-time PancakeSwap DEX data via Web3.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import real-time data fetching function from the module\n",
    "from src.pancake_predictor import fetch_live_market_data\n",
    "\n",
    "# Fetch real-time data from PancakeSwap DEX (no Binance, no simulation)\n",
    "print('Fetching real-time data from PancakeSwap DEX...')\n",
    "market_data, contract_info = fetch_live_market_data(\n",
    "    timeframe='1m',\n",
    "    limit=500,  # Fetch 500 minutes of data\n",
    "    use_contract=True  # Include prediction contract data\n",
    ")\n",
    "\n",
    "print(f'Data fetched: {len(market_data)} candles')\n",
    "print(f'Columns: {list(market_data.columns)}')\n",
    "market_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Preprocessing (Sequence Creation)\n",
    "\n",
    "We create sliding windows of 30 minutes to predict if price goes UP in the next 5 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(df):\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Normalize Data\n",
    "    scaler = MinMaxScaler()\n",
    "    scaled_data = scaler.fit_transform(df)\n",
    "\n",
    "    # Create Windows\n",
    "    data_val = df.values\n",
    "\n",
    "    for i in range(SEQ_LENGTH, len(df) - PREDICT_AHEAD):\n",
    "        # Input: Past 30 mins\n",
    "        X.append(scaled_data[i - SEQ_LENGTH:i])\n",
    "\n",
    "        # Target: Did price go UP in the next 5 mins?\n",
    "        current_price = data_val[i][0]  # Close is index 0\n",
    "        future_price = data_val[i + PREDICT_AHEAD][0]\n",
    "\n",
    "        label = 1 if future_price > current_price else 0\n",
    "        y.append(label)\n",
    "\n",
    "    return np.array(X), np.array(y), scaler\n",
    "\n",
    "print('>>> PREPARING SEQUENCES...')\n",
    "X, y, scaler = create_sequences(market_data)\n",
    "split = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split], X[split:]\n",
    "y_train, y_test = y[:split], y[split:]\n",
    "\n",
    "print(f'Training: X={X_train.shape}, y={y_train.shape}')\n",
    "print(f'Testing:  X={X_test.shape}, y={y_test.shape}')\n",
    "print(f'Label balance: {y_train.mean():.2%} Bull / {1 - y_train.mean():.2%} Bear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Model 1: Base Model (BiLSTM + Attention)\n",
    "\n",
    "The base architecture uses:\n",
    "- **BiLSTM (Momentum Detector)**: Reads the last 30 minutes bidirectionally. Strong \"Green Candle\" patterns with increasing volume bias towards Bull.\n",
    "- **MultiHeadAttention (Whale Detector)**: If a massive sell-off occurred 15 minutes ago, the Attention layer highlights that event.\n",
    "- **GlobalAveragePooling + Dense**: Condenses the sequence into a single Bull probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pancake_model():\n",
    "    \"\"\"Base Model: BiLSTM + Attention\"\"\"\n",
    "    inputs = Input(shape=(SEQ_LENGTH, FEATURES))\n",
    "\n",
    "    # Layer 1: BiLSTM (Momentum Detector)\n",
    "    # Reads minute-by-minute price action\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "\n",
    "    # Layer 2: Self-Attention (Whale Detector)\n",
    "    # Focuses on specific minutes with abnormal volume\n",
    "    attn = layers.MultiHeadAttention(num_heads=4, key_dim=32)(x, x)\n",
    "    x = layers.Add()([x, attn])\n",
    "    x = layers.LayerNormalization()(x)\n",
    "\n",
    "    # Layer 3: Decision\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid', name='Bull_Probability')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('>>> BUILDING BASE MODEL...')\n",
    "base_model = build_pancake_model()\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>>> TRAINING BASE MODEL...')\n",
    "base_history = base_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "base_loss, base_acc = base_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'\\nBase Model \u2014 Test Loss: {base_loss:.4f}, Test Accuracy: {base_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model 2: Robust Model (Conv1D + Stacked BiLSTM + Attention + Residual)\n",
    "\n",
    "The robust architecture adds several DLA layers on top of the base:\n",
    "\n",
    "| Layer | Purpose |\n",
    "|---|---|\n",
    "| **GaussianNoise(0.05)** | Prevents overfitting \u2014 the model learns the \"shape\" through the \"fog\" |\n",
    "| **Conv1D(32, kernel=3)** | Automatic candlestick pattern extraction (looks at 3 minutes at a time) |\n",
    "| **Stacked BiLSTM \u00d72** | Layer 1 captures fast patterns, Layer 2 captures deep trends |\n",
    "| **MultiHeadAttention + Residual** | Transformer block with ResNet-style skip connection |\n",
    "| **Dense(64) + Dropout(0.3)** | Heavy regularization for noisy financial data |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_robust_pancake_model():\n",
    "    \"\"\"Robust Model: Conv1D + Stacked BiLSTM + Attention + Residual\"\"\"\n",
    "    inputs = Input(shape=(SEQ_LENGTH, FEATURES))\n",
    "\n",
    "    # --- LAYER 1: ROBUSTNESS (Gaussian Noise) ---\n",
    "    # We inject random noise (stddev=0.05) to the input data.\n",
    "    # This prevents the model from memorizing exact prices (Overfitting).\n",
    "    # It learns to see the \"Shape\" through the \"Fog\".\n",
    "    x = GaussianNoise(0.05)(inputs)\n",
    "\n",
    "    # --- LAYER 2: FEATURE EXTRACTION (Conv1D) ---\n",
    "    # Filters=32, Kernel=3 means \"Look at 3 minutes at a time\".\n",
    "    # This automatically learns candlestick patterns (e.g., Engulfing candles).\n",
    "    x = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(x)\n",
    "    x = LayerNormalization()(x)  # Keeps values stable\n",
    "\n",
    "    # --- LAYER 3: TEMPORAL MEMORY (Stacked BiLSTMs) ---\n",
    "    # We stack two LSTMs.\n",
    "    # LSTM 1: Fast patterns (return_sequences=True keeps the timeline)\n",
    "    x = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "\n",
    "    # LSTM 2: Deep patterns (The \"Trend\")\n",
    "    # We save this output 'lstm_out' for the Residual connection later\n",
    "    lstm_out = layers.Bidirectional(layers.LSTM(64, return_sequences=True))(x)\n",
    "\n",
    "    # --- LAYER 4: ATTENTION + RESIDUAL (The Transformer Block) ---\n",
    "    # Multi-Head Attention looks for correlations across the 30-minute window\n",
    "    attn_out = layers.MultiHeadAttention(num_heads=4, key_dim=32)(lstm_out, lstm_out)\n",
    "\n",
    "    # RESIDUAL CONNECTION (Add & Norm)\n",
    "    # We add the LSTM memory (lstm_out) to the Attention insight (attn_out).\n",
    "    # This is the \"Safety Net\" that creates robust Deep Learning models (like ResNet).\n",
    "    x = Add()([lstm_out, attn_out])\n",
    "    x = LayerNormalization()(x)\n",
    "\n",
    "    # --- LAYER 5: DECISION ---\n",
    "    x = GlobalAveragePooling1D()(x)  # Summarize the whole sequence\n",
    "\n",
    "    # Dense layers to reason about the features\n",
    "    x = layers.Dense(64, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)  # Heavy dropout for financial data\n",
    "\n",
    "    output = layers.Dense(1, activation='sigmoid', name='Bull_Probability')(x)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    # Use a lower learning rate for robust fine-tuning\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.0005)\n",
    "\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "print('>>> BUILDING ROBUST MODEL...')\n",
    "robust_model = build_robust_pancake_model()\n",
    "robust_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Robust Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('>>> TRAINING ROBUST MODEL...')\n",
    "robust_history = robust_model.fit(\n",
    "    X_train, y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_test, y_test),\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "robust_loss, robust_acc = robust_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'\\nRobust Model \u2014 Test Loss: {robust_loss:.4f}, Test Accuracy: {robust_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Comparison\n",
    "\n",
    "Compare training curves and final metrics between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(16, 10))\n",
    "\n",
    "# --- Row 1: Accuracy ---\n",
    "axes[0, 0].plot(base_history.history['accuracy'], label='Base Train')\n",
    "axes[0, 0].plot(base_history.history['val_accuracy'], label='Base Val')\n",
    "axes[0, 0].axhline(y=0.53, color='r', linestyle='--', label='Break-Even (53%)')\n",
    "axes[0, 0].set_title('Base Model \u2014 Accuracy')\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Accuracy')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].plot(robust_history.history['accuracy'], label='Robust Train')\n",
    "axes[0, 1].plot(robust_history.history['val_accuracy'], label='Robust Val')\n",
    "axes[0, 1].axhline(y=0.53, color='r', linestyle='--', label='Break-Even (53%)')\n",
    "axes[0, 1].set_title('Robust Model \u2014 Accuracy')\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Accuracy')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# --- Row 2: Loss ---\n",
    "axes[1, 0].plot(base_history.history['loss'], label='Base Train')\n",
    "axes[1, 0].plot(base_history.history['val_loss'], label='Base Val')\n",
    "axes[1, 0].set_title('Base Model \u2014 Loss')\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Loss')\n",
    "axes[1, 0].legend()\n",
    "\n",
    "axes[1, 1].plot(robust_history.history['loss'], label='Robust Train')\n",
    "axes[1, 1].plot(robust_history.history['val_loss'], label='Robust Val')\n",
    "axes[1, 1].set_title('Robust Model \u2014 Loss')\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Loss')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.suptitle('Model Comparison: Base vs Robust', fontsize=14, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary table\n",
    "print('\\n' + '=' * 60)\n",
    "print(f'{\"Metric\":<25} {\"Base Model\":>15} {\"Robust Model\":>15}')\n",
    "print('=' * 60)\n",
    "print(f'{\"Test Loss\":<25} {base_loss:>15.4f} {robust_loss:>15.4f}')\n",
    "print(f'{\"Test Accuracy\":<25} {base_acc:>15.4f} {robust_acc:>15.4f}')\n",
    "print(f'{\"Final Train Loss\":<25} {base_history.history[\"loss\"][-1]:>15.4f} {robust_history.history[\"loss\"][-1]:>15.4f}')\n",
    "print(f'{\"Final Train Accuracy\":<25} {base_history.history[\"accuracy\"][-1]:>15.4f} {robust_history.history[\"accuracy\"][-1]:>15.4f}')\n",
    "print('=' * 60)\n",
    "\n",
    "winner = 'Base Model' if base_acc > robust_acc else 'Robust Model'\n",
    "print(f'\\n>>> Best single model: {winner}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Ensemble Prediction (Dual Model)\n",
    "\n",
    "Combine predictions from both models to reduce variance and improve reliability.\n",
    "The ensemble uses a weighted average of both model outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(base_model, robust_model, sequence, base_weight=0.5, robust_weight=0.5):\n",
    "    \"\"\"\n",
    "    Generate predictions from both models and combine them.\n",
    "    \"\"\"\n",
    "    seq_reshaped = sequence.reshape(1, SEQ_LENGTH, FEATURES)\n",
    "\n",
    "    base_prob = float(base_model.predict(seq_reshaped, verbose=0)[0][0])\n",
    "    robust_prob = float(robust_model.predict(seq_reshaped, verbose=0)[0][0])\n",
    "    ensemble_prob = (base_weight * base_prob) + (robust_weight * robust_prob)\n",
    "\n",
    "    def _decision(prob):\n",
    "        if prob > 0.60:\n",
    "            return 'BET BULL'\n",
    "        elif prob < 0.40:\n",
    "            return 'BET BEAR'\n",
    "        return 'SKIP'\n",
    "\n",
    "    return {\n",
    "        'base_prob': base_prob,\n",
    "        'robust_prob': robust_prob,\n",
    "        'ensemble_prob': ensemble_prob,\n",
    "        'base_decision': _decision(base_prob),\n",
    "        'robust_decision': _decision(robust_prob),\n",
    "        'ensemble_decision': _decision(ensemble_prob),\n",
    "    }\n",
    "\n",
    "# --- Test on multiple samples ---\n",
    "print('>>> ENSEMBLE PREDICTIONS ON TEST DATA ---\\n')\n",
    "print(f'{\"Sample\":<8} {\"Base Prob\":>10} {\"Robust Prob\":>12} {\"Ensemble\":>10} {\"Base\":>12} {\"Robust\":>12} {\"Ensemble\":>12} {\"Actual\":>8}')\n",
    "print('-' * 100)\n",
    "\n",
    "num_samples = 10\n",
    "indices = np.linspace(0, len(X_test) - 1, num_samples, dtype=int)\n",
    "\n",
    "correct_base = 0\n",
    "correct_robust = 0\n",
    "correct_ensemble = 0\n",
    "\n",
    "for idx in indices:\n",
    "    result = ensemble_predict(base_model, robust_model, X_test[idx])\n",
    "    actual = 'BULL' if y_test[idx] == 1 else 'BEAR'\n",
    "\n",
    "    # Check correctness\n",
    "    actual_bull = y_test[idx] == 1\n",
    "    if (result['base_prob'] > 0.5) == actual_bull:\n",
    "        correct_base += 1\n",
    "    if (result['robust_prob'] > 0.5) == actual_bull:\n",
    "        correct_robust += 1\n",
    "    if (result['ensemble_prob'] > 0.5) == actual_bull:\n",
    "        correct_ensemble += 1\n",
    "\n",
    "    print(f'{idx:<8} {result[\"base_prob\"]:>10.4f} {result[\"robust_prob\"]:>12.4f} {result[\"ensemble_prob\"]:>10.4f} '\n",
    "          f'{result[\"base_decision\"]:>12} {result[\"robust_decision\"]:>12} {result[\"ensemble_decision\"]:>12} {actual:>8}')\n",
    "\n",
    "print(f'\\nSample accuracy \u2014 Base: {correct_base}/{num_samples}, '\n",
    "      f'Robust: {correct_robust}/{num_samples}, '\n",
    "      f'Ensemble: {correct_ensemble}/{num_samples}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Trading Strategy (EV-Based Logic)\n",
    "\n",
    "### The \"Kelly Criterion\" (EV Logic)\n",
    "\n",
    "The `trade_logic` function is crucial. Sometimes the AI is unsure (51% Bull), but if the crowd is heavily betting Bear, the Bull Payout might be 2.5x.\n",
    "\n",
    "**EV = (0.51 \u00d7 2.5) - (0.49 \u00d7 1) = 0.78** \u2014 a massively profitable bet despite low confidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trade_logic(model, current_sequence, bull_payout, bear_payout):\n",
    "    \"\"\"\n",
    "    Decides whether to bet based on Model Confidence AND Pool Odds (EV).\n",
    "    \"\"\"\n",
    "    seq_reshaped = current_sequence.reshape(1, SEQ_LENGTH, FEATURES)\n",
    "    prob_bull = float(model.predict(seq_reshaped, verbose=0)[0][0])\n",
    "    prob_bear = 1.0 - prob_bull\n",
    "\n",
    "    decision = 'SKIP'\n",
    "\n",
    "    # Calculate Expected Value (EV)\n",
    "    ev_bull = (prob_bull * bull_payout) - (prob_bear * 1)\n",
    "    ev_bear = (prob_bear * bear_payout) - (prob_bull * 1)\n",
    "\n",
    "    # Thresholding (Only bet if we have an edge)\n",
    "    CONFIDENCE_THRESHOLD = 0.60\n",
    "\n",
    "    if ev_bull > 0.2 and prob_bull > CONFIDENCE_THRESHOLD:\n",
    "        decision = 'BET BULL'\n",
    "    elif ev_bear > 0.2 and prob_bear > CONFIDENCE_THRESHOLD:\n",
    "        decision = 'BET BEAR'\n",
    "\n",
    "    return decision, prob_bull, ev_bull, ev_bear\n",
    "\n",
    "# --- SIMULATE A LIVE ROUND ---\n",
    "print('\\n>>> LIVE ROUND PREDICTION (Base Model) ---')\n",
    "latest_data = X_test[-1]\n",
    "current_bull_payout = 1.95\n",
    "current_bear_payout = 1.95\n",
    "\n",
    "action, conf, ev_up, ev_down = trade_logic(base_model, latest_data, current_bull_payout, current_bear_payout)\n",
    "print(f'AI Bull Probability: {conf:.2%}')\n",
    "print(f'EV Bull: {ev_up:.2f} | EV Bear: {ev_down:.2f}')\n",
    "print(f'STRATEGY CALL: {action}')\n",
    "\n",
    "print('\\n>>> LIVE ROUND PREDICTION (Robust Model) ---')\n",
    "action2, conf2, ev_up2, ev_down2 = trade_logic(robust_model, latest_data, current_bull_payout, current_bear_payout)\n",
    "print(f'AI Bull Probability: {conf2:.2%}')\n",
    "print(f'EV Bull: {ev_up2:.2f} | EV Bear: {ev_down2:.2f}')\n",
    "print(f'STRATEGY CALL: {action2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Knowledge Distillation (Teacher \u2192 Student)\n",
    "\n",
    "Both trained models act as **teachers**. We create a smaller **student** model that learns\n",
    "from the combined soft predictions of both teachers. This transfers the learned knowledge\n",
    "of both architectures into a single lightweight model.\n",
    "\n",
    "Benefits:\n",
    "- Faster inference (smaller model)\n",
    "- Combines strengths of both architectures\n",
    "- Soft labels carry more information than hard labels (dark knowledge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_distilled_model(teacher_base, teacher_robust, X_train,\n",
    "                          epochs=5, batch_size=32):\n",
    "    \"\"\"\n",
    "    Build a student model that learns from both teacher models.\n",
    "    \"\"\"\n",
    "    # Generate soft labels from both teachers\n",
    "    print('>>> Generating soft labels from teachers...')\n",
    "    base_preds = teacher_base.predict(X_train, verbose=0)\n",
    "    robust_preds = teacher_robust.predict(X_train, verbose=0)\n",
    "    soft_labels = (base_preds + robust_preds) / 2.0\n",
    "\n",
    "    # Build a lightweight student model\n",
    "    inputs = Input(shape=(SEQ_LENGTH, FEATURES))\n",
    "    x = layers.Bidirectional(layers.LSTM(32, return_sequences=True))(inputs)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    attn = layers.MultiHeadAttention(num_heads=2, key_dim=16)(x, x)\n",
    "    x = Add()([x, attn])\n",
    "    x = LayerNormalization()(x)\n",
    "    x = GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dense(32, activation='relu')(x)\n",
    "    output = layers.Dense(1, activation='sigmoid', name='Bull_Probability')(x)\n",
    "\n",
    "    student = models.Model(inputs=inputs, outputs=output)\n",
    "    student.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='binary_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # Train on soft labels from teachers\n",
    "    print('>>> Training student on teacher knowledge...')\n",
    "    student.fit(X_train, soft_labels, epochs=epochs, batch_size=batch_size, verbose=1)\n",
    "\n",
    "    return student\n",
    "\n",
    "print('>>> KNOWLEDGE DISTILLATION...')\n",
    "student_model = build_distilled_model(base_model, robust_model, X_train)\n",
    "\n",
    "# Evaluate student on real labels\n",
    "student_loss, student_acc = student_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f'\\nStudent Model \u2014 Test Loss: {student_loss:.4f}, Test Accuracy: {student_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Final Comparison: All Three Models\n",
    "\n",
    "Compare the base model, robust model, and knowledge-distilled student model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full test-set evaluation\n",
    "base_preds_all = (base_model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "robust_preds_all = (robust_model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "student_preds_all = (student_model.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "\n",
    "# Ensemble predictions\n",
    "base_probs_all = base_model.predict(X_test, verbose=0).flatten()\n",
    "robust_probs_all = robust_model.predict(X_test, verbose=0).flatten()\n",
    "ensemble_probs_all = (base_probs_all + robust_probs_all) / 2.0\n",
    "ensemble_preds_all = (ensemble_probs_all > 0.5).astype(int)\n",
    "\n",
    "# Accuracy\n",
    "base_full_acc = np.mean(base_preds_all == y_test)\n",
    "robust_full_acc = np.mean(robust_preds_all == y_test)\n",
    "student_full_acc = np.mean(student_preds_all == y_test)\n",
    "ensemble_full_acc = np.mean(ensemble_preds_all == y_test)\n",
    "\n",
    "print('=' * 65)\n",
    "print('           FINAL MODEL COMPARISON (Full Test Set)')\n",
    "print('=' * 65)\n",
    "print(f'{\"Model\":<25} {\"Accuracy\":>12} {\"vs Break-Even\":>15}')\n",
    "print('-' * 65)\n",
    "print(f'{\"Base (BiLSTM+Attn)\":<25} {base_full_acc:>12.4f} {base_full_acc - 0.53:>+15.4f}')\n",
    "print(f'{\"Robust (Conv+BiLSTM)\":<25} {robust_full_acc:>12.4f} {robust_full_acc - 0.53:>+15.4f}')\n",
    "print(f'{\"Ensemble (Avg)\":<25} {ensemble_full_acc:>12.4f} {ensemble_full_acc - 0.53:>+15.4f}')\n",
    "print(f'{\"Student (Distilled)\":<25} {student_full_acc:>12.4f} {student_full_acc - 0.53:>+15.4f}')\n",
    "print('=' * 65)\n",
    "\n",
    "best_name = max(\n",
    "    [('Base', base_full_acc), ('Robust', robust_full_acc),\n",
    "     ('Ensemble', ensemble_full_acc), ('Student', student_full_acc)],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "print(f'\\n>>> BEST MODEL: {best_name[0]} ({best_name[1]:.4f})')\n",
    "\n",
    "# Visualization\n",
    "models_names = ['Base', 'Robust', 'Ensemble', 'Student']\n",
    "accuracies = [base_full_acc, robust_full_acc, ensemble_full_acc, student_full_acc]\n",
    "colors = ['#3498db', '#e74c3c', '#2ecc71', '#9b59b6']\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "bars = ax.bar(models_names, accuracies, color=colors, edgecolor='black')\n",
    "ax.axhline(y=0.53, color='red', linestyle='--', linewidth=2, label='Break-Even (53%)')\n",
    "ax.axhline(y=0.5, color='gray', linestyle=':', linewidth=1, label='Random (50%)')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Model Comparison: Accuracy on Test Set', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax.text(bar.get_x() + bar.get_width() / 2., bar.get_height() + 0.005,\n",
    "            f'{acc:.3f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Live Prediction Function\n",
    "\n",
    "A unified function that gets predictions from all models and recommends the best action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_round(sequence, bull_payout=1.95, bear_payout=1.95):\n",
    "    \"\"\"\n",
    "    Get predictions from all models for a single round.\n",
    "\n",
    "    Args:\n",
    "        sequence: Input array of shape (SEQ_LENGTH, FEATURES).\n",
    "        bull_payout: Current Bull payout multiplier.\n",
    "        bear_payout: Current Bear payout multiplier.\n",
    "\n",
    "    Returns:\n",
    "        Dict with predictions from base, robust, ensemble, and student models.\n",
    "    \"\"\"\n",
    "    seq = sequence.reshape(1, SEQ_LENGTH, FEATURES)\n",
    "\n",
    "    base_p = float(base_model.predict(seq, verbose=0)[0][0])\n",
    "    robust_p = float(robust_model.predict(seq, verbose=0)[0][0])\n",
    "    ensemble_p = (base_p + robust_p) / 2.0\n",
    "    student_p = float(student_model.predict(seq, verbose=0)[0][0])\n",
    "\n",
    "    def _ev_decision(prob):\n",
    "        prob_bear = 1.0 - prob\n",
    "        ev_bull = (prob * bull_payout) - (prob_bear * 1)\n",
    "        ev_bear = (prob_bear * bear_payout) - (prob * 1)\n",
    "        if ev_bull > 0.2 and prob > 0.60:\n",
    "            return 'BET BULL', ev_bull, ev_bear\n",
    "        elif ev_bear > 0.2 and prob_bear > 0.60:\n",
    "            return 'BET BEAR', ev_bull, ev_bear\n",
    "        return 'SKIP', ev_bull, ev_bear\n",
    "\n",
    "    results = {}\n",
    "    for name, prob in [('Base', base_p), ('Robust', robust_p),\n",
    "                       ('Ensemble', ensemble_p), ('Student', student_p)]:\n",
    "        dec, ev_b, ev_br = _ev_decision(prob)\n",
    "        results[name] = {\n",
    "            'probability': prob,\n",
    "            'decision': dec,\n",
    "            'ev_bull': ev_b,\n",
    "            'ev_bear': ev_br\n",
    "        }\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Demo ---\n",
    "print('>>> UNIFIED PREDICTION FOR LATEST ROUND ---\\n')\n",
    "results = predict_round(X_test[-1])\n",
    "\n",
    "print(f'{\"Model\":<12} {\"Bull Prob\":>10} {\"EV Bull\":>10} {\"EV Bear\":>10} {\"Decision\":>12}')\n",
    "print('-' * 60)\n",
    "for name, r in results.items():\n",
    "    print(f'{name:<12} {r[\"probability\"]:>10.4f} {r[\"ev_bull\"]:>10.2f} {r[\"ev_bear\"]:>10.2f} {r[\"decision\"]:>12}')\n",
    "\n",
    "# Consensus check\n",
    "decisions = [r['decision'] for r in results.values()]\n",
    "if all(d == decisions[0] for d in decisions):\n",
    "    print(f'\\n>>> ALL MODELS AGREE: {decisions[0]}')\n",
    "else:\n",
    "    print(f'\\n>>> MODELS DISAGREE \u2014 Consider SKIP for safety')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Backtesting Simulation (Real Data Feedback)\n",
    "\n",
    "Simulate trading on the test set to evaluate real P&L performance.\n",
    "This uses the actual test labels as \"real data feedback\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(model, X_test, y_test, bull_payout=1.95, bear_payout=1.95, name='Model'):\n",
    "    \"\"\"\n",
    "    Backtest a model on the test set. Returns cumulative P&L.\n",
    "    \"\"\"\n",
    "    balance = 0.0\n",
    "    trades = 0\n",
    "    wins = 0\n",
    "    history = [0.0]\n",
    "\n",
    "    preds = model.predict(X_test, verbose=0).flatten()\n",
    "\n",
    "    for i in range(len(y_test)):\n",
    "        prob_bull = float(preds[i])\n",
    "        prob_bear = 1.0 - prob_bull\n",
    "        actual_bull = y_test[i] == 1\n",
    "\n",
    "        ev_bull = (prob_bull * bull_payout) - (prob_bear * 1)\n",
    "        ev_bear = (prob_bear * bear_payout) - (prob_bull * 1)\n",
    "\n",
    "        # Only trade when EV is positive and confidence is high\n",
    "        if ev_bull > 0.2 and prob_bull > 0.60:\n",
    "            trades += 1\n",
    "            if actual_bull:\n",
    "                balance += (bull_payout - 1)  # Net profit\n",
    "                wins += 1\n",
    "            else:\n",
    "                balance -= 1  # Lost stake\n",
    "        elif ev_bear > 0.2 and prob_bear > 0.60:\n",
    "            trades += 1\n",
    "            if not actual_bull:\n",
    "                balance += (bear_payout - 1)  # Net profit\n",
    "                wins += 1\n",
    "            else:\n",
    "                balance -= 1  # Lost stake\n",
    "\n",
    "        history.append(balance)\n",
    "\n",
    "    win_rate = wins / trades if trades > 0 else 0\n",
    "    print(f'{name:<20} Trades: {trades:>5} | Wins: {wins:>5} | '\n",
    "          f'Win Rate: {win_rate:.2%} | P&L: {balance:>+8.2f}')\n",
    "\n",
    "    return history\n",
    "\n",
    "print('>>> BACKTESTING ALL MODELS ---\\n')\n",
    "base_pnl = backtest(base_model, X_test, y_test, name='Base')\n",
    "robust_pnl = backtest(robust_model, X_test, y_test, name='Robust')\n",
    "student_pnl = backtest(student_model, X_test, y_test, name='Student')\n",
    "\n",
    "# Plot P&L curves\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "ax.plot(base_pnl, label='Base Model', alpha=0.8)\n",
    "ax.plot(robust_pnl, label='Robust Model', alpha=0.8)\n",
    "ax.plot(student_pnl, label='Student Model', alpha=0.8)\n",
    "ax.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "ax.set_title('Backtesting: Cumulative P&L', fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel('Trade Index')\n",
    "ax.set_ylabel('Cumulative P&L (units)')\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Real-World Integration Steps",
    "",
    "1. **Web3.py**: Fetch `currentEpoch`, `lockPrice`, and `bullAmount`/`bearAmount` from the PancakeSwap Prediction Smart Contract.",
    "2. **PancakeSwap DEX**: Use Web3.py to query on-chain data to fetch real-time OHLCV data to feed the model.",
    "3. **Latency**: Run close to BSC nodes. The \"Lock\" happens instantly; your transaction needs to be confirmed 5-10 seconds before the lock.",
    "",
    "### Summary of Architectures",
    "",
    "| Component | Base Model | Robust Model | Student Model |",
    "|---|---|---|---|",
    "| Input Noise | \u2717 | GaussianNoise(0.05) | \u2717 |",
    "| Conv1D | \u2717 | 32 filters, kernel=3 | \u2717 |",
    "| BiLSTM Layers | 1\u00d764 | 2\u00d764 (stacked) | 1\u00d732 |",
    "| Attention | 4 heads, key=32 | 4 heads, key=32 + Residual | 2 heads, key=16 |",
    "| Dropout | 0.2 | 0.3 | 0.2 |",
    "| Learning Rate | Adam default | Adam 0.0005 | Adam 0.001 |",
    "| Training | Hard labels | Hard labels | Soft labels (distilled) |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}