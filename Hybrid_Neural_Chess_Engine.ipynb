{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# \ud83e\udde0 Hybrid Neural Chess Engine\n\n### Learning from Chess.com (Hikaru Nakamura) + Self-Play Reinforcement"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \u2705 SECTION 0 \u2014 Setup (Colab Compatible)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip -q install python-chess torch torchvision requests tqdm\n\nimport os\nimport io\nimport re\nimport json\nimport random\nimport requests\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport chess\nimport chess.pgn\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udccc SECTION 1 \u2014 BUSINESS UNDERSTANDING\n\nThis notebook shows a complete **hybrid chess-learning workflow**:\n\n- **Imitation learning** from real Chess.com games (e.g., Hikaru).\n- **Self-play reinforcement** for additional policy refinement.\n- **Policy + Value** models for move selection and board evaluation.\n- **CNN + Transformer** architecture for spatial + contextual reasoning."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udf10 SECTION 2 \u2014 DOWNLOAD REAL DATA FROM CHESS.COM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def get_chesscom_archives(username: str):\n    url = f\"https://api.chess.com/pub/player/{username}/games/archives\"\n    response = requests.get(url, timeout=30)\n    response.raise_for_status()\n    return response.json().get(\"archives\", [])\n\n\ndef download_chesscom_pgn(username: str, max_archives: int = 12, out_path: str = \"hikaru_chesscom.pgn\"):\n    archives = get_chesscom_archives(username)\n    # Download latest archives first\n    archives = archives[::-1][:max_archives]\n\n    if not archives:\n        raise ValueError(f\"No archives found for user: {username}\")\n\n    all_pgn_chunks = []\n    for archive_url in tqdm(archives, desc=\"Downloading monthly PGNs\"):\n        pgn_url = archive_url + \"/pgn\"\n        r = requests.get(pgn_url, timeout=60)\n        if r.status_code == 200 and r.text.strip():\n            all_pgn_chunks.append(r.text.strip())\n\n    if not all_pgn_chunks:\n        raise ValueError(\"Could not download any PGN data from Chess.com archives.\")\n\n    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\n\n\".join(all_pgn_chunks))\n\n    return out_path, len(all_pgn_chunks)\n\n\n# Example: download Hikaru's latest archives\npgn_path, months_downloaded = download_chesscom_pgn(\"hikaru\", max_archives=12, out_path=\"hikaru_chesscom.pgn\")\nprint(f\"Saved PGN to: {pgn_path} | months downloaded: {months_downloaded}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca SECTION 3 \u2014 DATA PREPARATION"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def board_to_tensor(board: chess.Board) -> torch.Tensor:\n    tensor = np.zeros((12, 8, 8), dtype=np.float32)\n    for square, piece in board.piece_map().items():\n        row = 7 - chess.square_rank(square)\n        col = chess.square_file(square)\n        piece_type = piece.piece_type - 1\n        color_offset = 0 if piece.color == chess.WHITE else 6\n        tensor[piece_type + color_offset, row, col] = 1.0\n    return torch.tensor(tensor)\n\n\ndef generate_move_vocab():\n    files = \"abcdefgh\"\n    ranks = \"12345678\"\n    promotions = [\"q\", \"r\", \"b\", \"n\"]\n\n    moves = set()\n    for ff in files:\n        for fr in ranks:\n            for tf in files:\n                for tr in ranks:\n                    if ff == tf and fr == tr:\n                        continue\n                    base = f\"{ff}{fr}{tf}{tr}\"\n                    moves.add(base)\n                    if (fr == \"7\" and tr == \"8\") or (fr == \"2\" and tr == \"1\"):\n                        for p in promotions:\n                            moves.add(base + p)\n\n    moves = sorted(moves)\n    move_to_idx = {m: i for i, m in enumerate(moves)}\n    idx_to_move = {i: m for m, i in move_to_idx.items()}\n    return moves, move_to_idx, idx_to_move\n\n\nall_moves, move_to_idx, idx_to_move = generate_move_vocab()\nprint(\"Move vocab size:\", len(all_moves))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udce6 SECTION 4 \u2014 DATASET CLASS (PGN)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class ChessDataset(Dataset):\n    def __init__(self, pgn_file: str, move_to_idx: dict, max_games: int | None = 1000):\n        self.positions = []\n        self.moves = []\n\n        loaded_games = 0\n        with open(pgn_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n            while True:\n                game = chess.pgn.read_game(f)\n                if game is None:\n                    break\n\n                board = game.board()\n                for move in game.mainline_moves():\n                    self.positions.append(board_to_tensor(board))\n                    self.moves.append(move_to_idx.get(move.uci(), 0))\n                    board.push(move)\n\n                loaded_games += 1\n                if max_games is not None and loaded_games >= max_games:\n                    break\n\n        self.moves = torch.tensor(self.moves, dtype=torch.long)\n        print(f\"Loaded {loaded_games} games, {len(self.positions)} positions\")\n\n    def __len__(self):\n        return len(self.positions)\n\n    def __getitem__(self, idx):\n        return self.positions[idx], self.moves[idx]\n\n\ndataset = ChessDataset(pgn_path, move_to_idx, max_games=500)\ndataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83e\udde0 SECTION 5 \u2014 MODEL ARCHITECTURE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "class PolicyNetwork(nn.Module):\n    def __init__(self, move_vocab_size: int):\n        super().__init__()\n        self.cnn = nn.Sequential(\n            nn.Conv2d(12, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 128, 3, padding=1),\n            nn.ReLU(),\n            nn.Flatten()\n        )\n        self.fc = nn.Linear(128 * 8 * 8, 512)\n\n        enc_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n        self.transformer = nn.TransformerEncoder(enc_layer, num_layers=2)\n\n        self.policy_head = nn.Linear(512, move_vocab_size)\n\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.fc(x)\n        x = x.unsqueeze(1)\n        x = self.transformer(x)\n        x = x.squeeze(1)\n        return self.policy_head(x)\n\n\nclass ValueNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(12, 64, 3, padding=1),\n            nn.ReLU(),\n            nn.Flatten(),\n            nn.Linear(64 * 8 * 8, 256),\n            nn.ReLU(),\n            nn.Linear(256, 1),\n            nn.Tanh()\n        )\n\n    def forward(self, x):\n        return self.model(x)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udfaf SECTION 6 \u2014 IMITATION LEARNING"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\npolicy_net = PolicyNetwork(len(move_to_idx)).to(device)\nvalue_net = ValueNetwork().to(device)\n\noptimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\ncriterion = nn.CrossEntropyLoss()\n\n\ndef train_policy(dataloader, epochs=2):\n    policy_net.train()\n    for epoch in range(epochs):\n        total_loss = 0.0\n        for boards, moves in tqdm(dataloader, desc=f\"Epoch {epoch+1}\"):\n            boards = boards.to(device)\n            moves = moves.to(device)\n\n            logits = policy_net(boards)\n            loss = criterion(logits, moves)\n\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            total_loss += loss.item()\n\n        print(f\"Epoch {epoch+1}/{epochs} | loss={total_loss:.4f}\")\n\n\ntrain_policy(dataloader, epochs=2)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udd25 SECTION 7 \u2014 SELF-PLAY REINFORCEMENT (SIMPLIFIED)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def select_legal_move_from_logits(board: chess.Board, logits: torch.Tensor):\n    legal = list(board.legal_moves)\n    legal_indices = [move_to_idx[m.uci()] for m in legal if m.uci() in move_to_idx]\n    if not legal_indices:\n        move = random.choice(legal)\n        return move, move_to_idx.get(move.uci(), 0)\n\n    local_logits = logits[0, legal_indices]\n    probs = torch.softmax(local_logits, dim=0)\n    selected_local = torch.multinomial(probs, 1).item()\n    selected_idx = legal_indices[selected_local]\n    return chess.Move.from_uci(idx_to_move[selected_idx]), selected_idx\n\n\ndef self_play_game(max_plies=200):\n    board = chess.Board()\n    history = []\n    policy_net.eval()\n\n    while not board.is_game_over() and len(history) < max_plies:\n        state = board_to_tensor(board).unsqueeze(0).to(device)\n        with torch.no_grad():\n            logits = policy_net(state)\n        move, move_idx = select_legal_move_from_logits(board, logits)\n        history.append((state, move_idx))\n        board.push(move)\n\n    result = board.result()\n    reward = 1 if result == \"1-0\" else -1 if result == \"0-1\" else 0\n    return history, reward, result\n\n\nfor i in range(3):\n    _, reward, result = self_play_game()\n    print(f\"Self-play game {i+1}: {result}, reward={reward}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udfae SECTION 8 \u2014 PLAY AGAINST ENGINE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def predict_move(board: chess.Board) -> chess.Move:\n    state = board_to_tensor(board).unsqueeze(0).to(device)\n    policy_net.eval()\n    with torch.no_grad():\n        logits = policy_net(state)\n\n    legal = list(board.legal_moves)\n    legal_indices = [move_to_idx[m.uci()] for m in legal if m.uci() in move_to_idx]\n    if not legal_indices:\n        return random.choice(legal)\n\n    local_logits = logits[0, legal_indices]\n    best_local = torch.argmax(local_logits).item()\n    best_idx = legal_indices[best_local]\n    return chess.Move.from_uci(idx_to_move[best_idx])\n\n\nboard = chess.Board()\nprint(\"Engine move from start:\", predict_move(board).uci())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca SECTION 9 \u2014 EVALUATION\n\n- Top-1 and Top-5 move prediction on held-out Chess.com games.\n- Win rate vs random legal-move baseline.\n- Optional Elo tracking by checkpoint."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}