{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# \ud83e\udde0 Hybrid Neural Chess Engine (Keras)\n\n### Learning from Chess.com (Hikaru Nakamura) + Self-Play Reinforcement"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \u2705 SECTION 0 \u2014 Setup (Colab Compatible)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "!pip -q install python-chess tensorflow requests tqdm\n\nimport os\nimport random\nimport requests\nimport numpy as np\nfrom tqdm.auto import tqdm\n\nimport chess\nimport chess.pgn\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\n\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntf.random.set_seed(SEED)\nprint('TensorFlow:', tf.__version__)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udccc SECTION 1 \u2014 BUSINESS UNDERSTANDING\n\nThis notebook demonstrates the same hybrid logic in **Keras**:\n- Imitation learning on real Chess.com games.\n- Self-play reinforcement-style rollout for policy improvement loops.\n- Policy + Value split.\n- CNN + Transformer architecture for spatial/contextual reasoning."
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udf10 SECTION 2 \u2014 DOWNLOAD REAL DATA FROM CHESS.COM"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def get_chesscom_archives(username: str):\n    url = f\"https://api.chess.com/pub/player/{username}/games/archives\"\n    response = requests.get(url, timeout=30)\n    response.raise_for_status()\n    return response.json().get(\"archives\", [])\n\n\ndef download_chesscom_pgn(username: str, max_archives: int = 12, out_path: str = \"hikaru_chesscom.pgn\"):\n    archives = get_chesscom_archives(username)\n    archives = archives[::-1][:max_archives]  # latest first\n\n    if not archives:\n        raise ValueError(f\"No archives found for user: {username}\")\n\n    all_pgn_chunks = []\n    for archive_url in tqdm(archives, desc=\"Downloading monthly PGNs\"):\n        pgn_url = archive_url + \"/pgn\"\n        r = requests.get(pgn_url, timeout=60)\n        if r.status_code == 200 and r.text.strip():\n            all_pgn_chunks.append(r.text.strip())\n\n    if not all_pgn_chunks:\n        raise ValueError(\"Could not download PGN data from Chess.com archives.\")\n\n    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n        f.write(\"\\n\\n\".join(all_pgn_chunks))\n\n    return out_path, len(all_pgn_chunks)\n\n\npgn_path, months_downloaded = download_chesscom_pgn(\"hikaru\", max_archives=12, out_path=\"hikaru_chesscom.pgn\")\nprint(f\"Saved PGN to: {pgn_path} | months downloaded: {months_downloaded}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca SECTION 3 \u2014 DATA PREPARATION"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def board_to_tensor(board: chess.Board) -> np.ndarray:\n    # Channel-first for easier chess encoding; we'll transpose to channel-last for Keras.\n    tensor = np.zeros((12, 8, 8), dtype=np.float32)\n    for square, piece in board.piece_map().items():\n        row = 7 - chess.square_rank(square)\n        col = chess.square_file(square)\n        piece_type = piece.piece_type - 1\n        color_offset = 0 if piece.color == chess.WHITE else 6\n        tensor[piece_type + color_offset, row, col] = 1.0\n    return tensor\n\n\ndef generate_move_vocab():\n    files = \"abcdefgh\"\n    ranks = \"12345678\"\n    promotions = [\"q\", \"r\", \"b\", \"n\"]\n\n    moves = set()\n    for ff in files:\n        for fr in ranks:\n            for tf in files:\n                for tr in ranks:\n                    if ff == tf and fr == tr:\n                        continue\n                    base = f\"{ff}{fr}{tf}{tr}\"\n                    moves.add(base)\n                    if (fr == \"7\" and tr == \"8\") or (fr == \"2\" and tr == \"1\"):\n                        for p in promotions:\n                            moves.add(base + p)\n\n    moves = sorted(moves)\n    move_to_idx = {m: i for i, m in enumerate(moves)}\n    idx_to_move = {i: m for m, i in move_to_idx.items()}\n    return moves, move_to_idx, idx_to_move\n\n\nall_moves, move_to_idx, idx_to_move = generate_move_vocab()\nprint('Move vocab size:', len(all_moves))\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udce6 SECTION 4 \u2014 DATASET BUILD (PGN -> NUMPY)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def load_positions_from_pgn(pgn_file: str, max_games: int = 500):\n    X, y = [], []\n    loaded_games = 0\n\n    with open(pgn_file, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n        while True:\n            game = chess.pgn.read_game(f)\n            if game is None:\n                break\n\n            board = game.board()\n            for mv in game.mainline_moves():\n                encoded = board_to_tensor(board)              # (12,8,8)\n                encoded = np.transpose(encoded, (1, 2, 0))   # (8,8,12) for Keras Conv2D\n                X.append(encoded)\n                y.append(move_to_idx.get(mv.uci(), 0))\n                board.push(mv)\n\n            loaded_games += 1\n            if max_games is not None and loaded_games >= max_games:\n                break\n\n    X = np.asarray(X, dtype=np.float32)\n    y = np.asarray(y, dtype=np.int32)\n    print(f\"Loaded games: {loaded_games}, positions: {len(X)}\")\n    return X, y\n\n\nX, y = load_positions_from_pgn(pgn_path, max_games=500)\n\n# train/val split\nsplit = int(0.9 * len(X)) if len(X) > 1 else len(X)\nX_train, y_train = X[:split], y[:split]\nX_val, y_val = X[split:], y[split:]\nprint('Train:', X_train.shape, 'Val:', X_val.shape)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83e\udde0 SECTION 5 \u2014 KERAS TRANSFORMER ARCHITECTURE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def transformer_block(x, num_heads=8, ff_dim=512, dropout=0.1):\n    attn_out = layers.MultiHeadAttention(num_heads=num_heads, key_dim=x.shape[-1])(x, x)\n    attn_out = layers.Dropout(dropout)(attn_out)\n    x = layers.LayerNormalization(epsilon=1e-6)(x + attn_out)\n\n    ff = layers.Dense(ff_dim, activation='relu')(x)\n    ff = layers.Dense(x.shape[-1])(ff)\n    ff = layers.Dropout(dropout)(ff)\n    return layers.LayerNormalization(epsilon=1e-6)(x + ff)\n\n\ndef build_policy_model(move_vocab_size: int):\n    inp = keras.Input(shape=(8, 8, 12), name='board')\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(inp)\n    x = layers.Conv2D(128, 3, padding='same', activation='relu')(x)\n\n    # Convert spatial map to sequence of 64 tokens\n    x = layers.Reshape((64, 128))(x)\n    x = layers.Dense(256)(x)\n\n    x = transformer_block(x, num_heads=8, ff_dim=512)\n    x = transformer_block(x, num_heads=8, ff_dim=512)\n\n    x = layers.GlobalAveragePooling1D()(x)\n    logits = layers.Dense(move_vocab_size, name='policy_logits')(x)\n\n    model = keras.Model(inp, logits, name='policy_model')\n    model.compile(\n        optimizer=keras.optimizers.Adam(1e-3),\n        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n        metrics=['accuracy']\n    )\n    return model\n\n\ndef build_value_model():\n    inp = keras.Input(shape=(8, 8, 12), name='board')\n    x = layers.Conv2D(64, 3, padding='same', activation='relu')(inp)\n    x = layers.Flatten()(x)\n    x = layers.Dense(256, activation='relu')(x)\n    out = layers.Dense(1, activation='tanh', name='value')(x)\n\n    model = keras.Model(inp, out, name='value_model')\n    model.compile(optimizer=keras.optimizers.Adam(1e-3), loss='mse')\n    return model\n\n\npolicy_model = build_policy_model(len(move_to_idx))\nvalue_model = build_value_model()\npolicy_model.summary()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udfaf SECTION 6 \u2014 IMITATION LEARNING"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "if len(X_train) > 0:\n    history = policy_model.fit(\n        X_train, y_train,\n        validation_data=(X_val, y_val) if len(X_val) > 0 else None,\n        epochs=2,\n        batch_size=32,\n        verbose=1\n    )\nelse:\n    print('Dataset is empty. Check PGN download and parsing.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udd25 SECTION 7 \u2014 SELF-PLAY REINFORCEMENT (SIMPLIFIED)"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def board_to_keras_input(board: chess.Board) -> np.ndarray:\n    arr = board_to_tensor(board)\n    arr = np.transpose(arr, (1, 2, 0))\n    return np.expand_dims(arr, axis=0).astype(np.float32)\n\n\ndef select_legal_move_from_logits(board: chess.Board, logits: np.ndarray):\n    legal = list(board.legal_moves)\n    legal_indices = [move_to_idx[m.uci()] for m in legal if m.uci() in move_to_idx]\n    if not legal_indices:\n        mv = random.choice(legal)\n        return mv, move_to_idx.get(mv.uci(), 0)\n\n    local_logits = logits[0, legal_indices]\n    probs = tf.nn.softmax(local_logits).numpy()\n    probs = probs / probs.sum()\n    local_choice = np.random.choice(len(legal_indices), p=probs)\n    move_idx = legal_indices[local_choice]\n    return chess.Move.from_uci(idx_to_move[move_idx]), move_idx\n\n\ndef self_play_game(max_plies=200):\n    board = chess.Board()\n    trajectory = []\n\n    while not board.is_game_over() and len(trajectory) < max_plies:\n        state = board_to_keras_input(board)\n        logits = policy_model.predict(state, verbose=0)\n        move, move_idx = select_legal_move_from_logits(board, logits)\n        trajectory.append((state, move_idx, board.turn))\n        board.push(move)\n\n    result = board.result()\n    reward = 1 if result == '1-0' else -1 if result == '0-1' else 0\n    return trajectory, reward, result\n\n\nfor i in range(3):\n    _, reward, result = self_play_game()\n    print(f'Self-play game {i+1}: {result}, reward={reward}')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83c\udfae SECTION 8 \u2014 PLAY AGAINST ENGINE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "def predict_move(board: chess.Board) -> chess.Move:\n    state = board_to_keras_input(board)\n    logits = policy_model.predict(state, verbose=0)\n\n    legal = list(board.legal_moves)\n    legal_indices = [move_to_idx[m.uci()] for m in legal if m.uci() in move_to_idx]\n    if not legal_indices:\n        return random.choice(legal)\n\n    local_logits = logits[0, legal_indices]\n    best_local = int(np.argmax(local_logits))\n    best_idx = legal_indices[best_local]\n    return chess.Move.from_uci(idx_to_move[best_idx])\n\n\nboard = chess.Board()\nprint('Engine move from start:', predict_move(board).uci())\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## \ud83d\udcca SECTION 9 \u2014 EVALUATION\n\n- Top-1 and Top-5 move prediction on held-out Chess.com games.\n- Win rate vs random legal-move baseline.\n- Optional Elo-like checkpoint tracking."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}